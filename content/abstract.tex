Manual modeling of material parameters and 3D geometry is a time-consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material predictions onto reconstructed 3D geometry remains a significant challenge.

In this thesis, we propose a framework for fusing 2D material data into 3D geometry using a combination of learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. These predictions are then integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy, we further introduce a neural refinement step using a multilayer perceptron (MLP), which takes ray-traced material features as input and produces detailed adjustments.

Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines. 